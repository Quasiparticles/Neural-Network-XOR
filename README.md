#Neural Network XOR

##Overview
This is an implementation of a two-layer neural network. The training method is stochastic (online) [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) with momentum. It computes XOR for the given input. It uses two activation functions, one for each layer. One is a tanh function and the other is the sigmoid function. It uses [cross-entropy](http://neuralnetworksanddeeplearning.com/chap3.html) as it's loss function. 

##Dependencies

None!

##Usage

Just run the following in terminal to see it run. 

``
python neuralnetwork_xor.py
``

##Credits

The credits for the majority of this code go to [lightcaster](https://github.com/lightcaster).
